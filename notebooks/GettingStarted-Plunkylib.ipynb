{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plunkylib creates its files in the current working directory, so let's change to the parent directory if we're in the notebooks directory, since we want to use the default samples\n",
    "import os\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "# we also want to ensure we have a .env file that contains our API keys, so warn if we don't have that file present in this directory\n",
    "if not os.path.exists(\".env\"):\n",
    "    print(\"WARNING: No .env file found in this directory. Please create one with the API Key contents mentioned in the README.md file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plunkylib import *\n",
    "import dataclasses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionParams(name='ExampleGPT3', engine='text-davinci-002', n=1, best_of=1, max_tokens=300, stop=['\\n\\n', '----\\n'], temperature=0.3, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0)\n",
      "CompletionParams(name='MyFirstEngine', engine='text-davinci-002', n=1, best_of=1, max_tokens=300, stop=['\\n\\n', '----\\n'], temperature=0.3, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0)\n"
     ]
    }
   ],
   "source": [
    "# with datafiles, you can look up an existing datafile object by name\n",
    "\n",
    "# lets mimic the readme but using the objects instead of the CLI\n",
    "example_params_to_copy = CompletionParams.objects.get(\"ExampleGPT3\")\n",
    "print(example_params_to_copy)\n",
    "\n",
    "# make a copy and replace the name\n",
    "params_myfirstengine = dataclasses.replace(example_params_to_copy, name=\"MyFirstEngine\")\n",
    "print(params_myfirstengine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionParams(name='MyFirstEngine', engine='text-davinci-002', n=1, best_of=1, max_tokens=100, stop=['\\n\\n', '----\\n'], temperature=0.3, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Bonus: If you wanted, you can change the parameters and the file will change accordingly\n",
    "params_myfirstengine.max_tokens = 100\n",
    "print(params_myfirstengine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt(name='ExampleSimple', text='Describe the color of the sky in flowery language:\\n----\\nDESCRIPTION:\\n')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prompt(name='MyFirstPrompt', text='Describe the color of the sky in flowery language:\\n----\\nDESCRIPTION:\\n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same thing with the prompt\n",
    "example_prompt_to_copy = Prompt.objects.get(\"ExampleSimple\")\n",
    "print(example_prompt_to_copy)\n",
    "\n",
    "# make a copy and replace the name\n",
    "prompt_myfirstprompt = dataclasses.replace(example_prompt_to_copy, name=\"MyFirstPrompt\")\n",
    "prompt_myfirstprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Petition(name='MyFirstPetition', prompt_name='MyFirstPrompt', params_name='MyFirstEngine', promptvars_name=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now a new Petition using the names of the prompt and params\n",
    "petition_example = Petition(\"MyFirstPetition\", prompt_myfirstprompt.name, params_myfirstengine.name)\n",
    "petition_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 10:56:48.895 | DEBUG    | chronological:_completion:36 - CONFIG:\n",
      "    Prompt: 'Describe the color of the sky in flowery language:\\n----\\nDESCRIPTION:\\n'\n",
      "    Temperature: 0.3\n",
      "    Engine: text-davinci-002\n",
      "    Max Tokens: 100\n",
      "    Top-P: 1.0\n",
      "    Stop: ['\\n\\n', '----\\n']\n",
      "    Presence Penalty 0.0\n",
      "    Frequency Penalty: 0.0\n",
      "    Echo: False\n",
      "    N: 1\n",
      "    Stream: False\n",
      "    Log-Probs: None\n",
      "    Best Of: 1\n",
      "    Logit Bias: {}\n",
      "2022-06-20 10:56:50.236 | DEBUG    | chronological:_completion:66 - GPT-3 Completion Result: {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\nThe sky is a beautiful blue color. It looks like a big blue ocean that you could dive into.\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1655740609,\n",
      "  \"id\": \"cmpl-5LE3djoubDPaKAw3GcBhGN6nyyKcl\",\n",
      "  \"model\": \"text-davinci-002\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# petitions have some lazy references that we tend to load right after if we want to complete with them\n",
    "petition_example.load_all()\n",
    "\n",
    "example_completion, adj_prompt_text = await petition_completion2(petition_example, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky is a beautiful blue color. It looks like a big blue ocean that you could dive into.\n"
     ]
    }
   ],
   "source": [
    "# Completion object contains our results\n",
    "print(example_completion.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can change the prompt, if we want\n",
    "prompt_myfirstprompt.text = \"Describe, in great detail, the smell of the beach after an oil spill.\\n\\nDESCRIPTION:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 10:57:07.808 | DEBUG    | chronological:_completion:36 - CONFIG:\n",
      "    Prompt: 'Describe, in great detail, the smell of the beach after an oil spill.\\n\\nDESCRIPTION:\\n'\n",
      "    Temperature: 0.3\n",
      "    Engine: text-davinci-002\n",
      "    Max Tokens: 100\n",
      "    Top-P: 1.0\n",
      "    Stop: ['\\n\\n', '----\\n']\n",
      "    Presence Penalty 0.0\n",
      "    Frequency Penalty: 0.0\n",
      "    Echo: False\n",
      "    N: 1\n",
      "    Stream: False\n",
      "    Log-Probs: None\n",
      "    Best Of: 1\n",
      "    Logit Bias: {}\n",
      "2022-06-20 10:57:10.886 | DEBUG    | chronological:_completion:66 - GPT-3 Completion Result: {\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\nThe beach smells like a mixture of oil and salt water. There is a strong smell of petroleum and a hint of seaweed. The oil has a thick, greasy texture and clings to the sand.\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1655740628,\n",
      "  \"id\": \"cmpl-5LE3w5Hhc7duHrQNYDP7EeyxncYvx\",\n",
      "  \"model\": \"text-davinci-002\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beach smells like a mixture of oil and salt water. There is a strong smell of petroleum and a hint of seaweed. The oil has a thick, greasy texture and clings to the sand.\n"
     ]
    }
   ],
   "source": [
    "#And you don't need to rebuild a new petition, the original one works great\n",
    "#You keep reusing it, and you can edit the prompt (or engine parameters) from code or from the text/yaml files\n",
    "example_completion2, adj_prompt_text = await petition_completion2(petition_example, {})\n",
    "\n",
    "# Completion object contains our results\n",
    "print(example_completion2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Petition(name='MySecondPetition', prompt_name='MyFirstPrompt', params_name='MySecondEngine', promptvars_name=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We could run the same thing with Jurassic, we just need new CompletionParams and a new petition that uses our prompt\n",
    "sample_jparams = CompletionParams.objects.get(\"ExampleJurassicJumbo\")\n",
    "params_mysecondengine = dataclasses.replace(sample_jparams, name=\"MySecondEngine\")\n",
    "params_mysecondengine.max_tokens = 100\n",
    "\n",
    "# We don't need a new prompt, we can use both\n",
    "petition_example2 = Petition(\"MySecondPetition\", prompt_myfirstprompt.name, params_mysecondengine.name)\n",
    "petition_example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beach smells like oil. It smells like oil, and dead fish, and dead birds, and dead crabs, and dead dolphins, and dead whales, and dead people. It smells like oil, and dead fish, and dead birds, and dead crabs, and dead dolphins, and dead whales, and dead people. It smells like oil, and dead fish, and dead birds, and dead crabs, and dead dolphins, and dead whales, and dead people. It smells like oil, and dead fish, and dead\n"
     ]
    }
   ],
   "source": [
    "params_mysecondengine.max_tokens = 100\n",
    "example_completion3, adj_prompt_text = await petition_completion2(petition_example2, {})\n",
    "\n",
    "# Completion object contains our results\n",
    "print(example_completion3.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa9af9f86d1d854ae0204e4baada49d6c0051b058ab6ea52d61832c609c5af7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
