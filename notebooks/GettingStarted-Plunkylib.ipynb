{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plunkylib creates its files in the current working directory, so let's change to the parent directory if we're in the notebooks directory, since we want to use the default samples\n",
    "import os\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "# we also want to ensure we have a .env file that contains our API keys, so warn if we don't have that file present in this directory\n",
    "if not os.path.exists(\".env\"):\n",
    "    print(\"WARNING: No .env file found in this directory. Please create one with the API Key contents mentioned in the README.md file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plunkylib import *\n",
    "import dataclasses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with datafiles, you can look up an existing datafile object by name\n",
    "\n",
    "# lets mimic the readme but using the objects instead of the CLI\n",
    "example_params_to_copy = CompletionParams.objects.get(\"ExampleGPT3\")\n",
    "print(example_params_to_copy)\n",
    "\n",
    "# make a copy and replace the name\n",
    "params_myfirstengine = dataclasses.replace(example_params_to_copy, name=\"MyFirstEngine\")\n",
    "print(params_myfirstengine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: If you wanted, you can change the parameters and the file will change accordingly\n",
    "params_myfirstengine.max_tokens = 100\n",
    "print(params_myfirstengine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing with the prompt\n",
    "example_prompt_to_copy = Prompt.objects.get(\"ExampleSimple\")\n",
    "print(example_prompt_to_copy)\n",
    "\n",
    "# make a copy and replace the name\n",
    "prompt_myfirstprompt = dataclasses.replace(example_prompt_to_copy, name=\"MyFirstPrompt\")\n",
    "prompt_myfirstprompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now a new Petition using the names of the prompt and params\n",
    "petition_example = Petition(\"MyFirstPetition\", prompt_myfirstprompt.name, params_myfirstengine.name)\n",
    "petition_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# petitions have some lazy references that we tend to load right after if we want to complete with them\n",
    "petition_example.load_all()\n",
    "\n",
    "example_completion, adj_prompt_text = await petition_completion2(petition=petition_example, additional={}, content_filter_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completion object contains our results\n",
    "print(example_completion.text)\n",
    "print(example_completion.content_filter_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can change the prompt, if we want\n",
    "prompt_myfirstprompt.text = \"Describe, in great detail, the smell of the beach after an oil spill.\\n\\nDESCRIPTION:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And you don't need to rebuild a new petition, the original one works great\n",
    "#You keep reusing it, and you can edit the prompt (or engine parameters) from code or from the text/yaml files\n",
    "example_completion2, adj_prompt_text = await petition_completion2(petition=petition_example, additional={}, content_filter_check=True)\n",
    "\n",
    "# Completion object contains our results\n",
    "print(example_completion2.text)\n",
    "print(example_completion2.content_filter_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could run the same thing with Jurassic, we just need new CompletionParams and a new petition that uses our prompt\n",
    "sample_jparams = CompletionParams.objects.get(\"ExampleJurassicJumbo\")\n",
    "params_mysecondengine = dataclasses.replace(sample_jparams, name=\"MySecondEngine\")\n",
    "params_mysecondengine.max_tokens = 100\n",
    "\n",
    "# We don't need a new prompt, we can use both\n",
    "petition_example2 = Petition(\"MySecondPetition\", prompt_myfirstprompt.name, params_mysecondengine.name)\n",
    "petition_example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_mysecondengine.max_tokens = 100\n",
    "example_completion3, adj_prompt_text = await petition_completion2(petition=petition_example2, additional={}, content_filter_check=True)\n",
    "\n",
    "# Completion object contains our results\n",
    "print(example_completion3.text)\n",
    "print(example_completion3.content_filter_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a chat prompt\n",
    "petition_example3 = Petition.objects.get(\"ExampleChat\")\n",
    "petition_example3.load_all()\n",
    "example_completion3, adj_prompt_text = await petition_completion2(petition=petition_example3, additional={}, content_filter_check=False)\n",
    "print(example_completion3.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51fa8440433762ef133fff635e4fbd401bf47680297175c70a3ef43135f7b4dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
