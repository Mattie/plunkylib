{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plunkylib creates its files in the current working directory, so let's change to the parent directory if we're in the notebooks directory, since we want to use the default samples\n",
    "import os\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "# we also want to ensure we have a .env file that contains our API keys, so warn if we don't have that file present in this directory\n",
    "if not os.path.exists(\".env\"):\n",
    "    print(\"WARNING: No .env file found in this directory. Please create one with the API Key contents mentioned in the README.md file.\")\n",
    "\n",
    "from plunkylib.aiwrapper import *\n",
    "from plunkylib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"What monsters are dangerous?\"\n",
    "x = await search_query(test_prompt, engine=\"pinecone\", index=\"idx-alexandria2\", top_k=3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = VectorSearchParams(\"ExamplePineconeParams\", engine=\"pinecone\", index=\"idx-alexandria2\", embedding=\"ExampleGPT3_Embedding\", top_k=3, include_metadata=True, include_values=True, result_format=\"\\n---\\n{result}\", filter=None)\n",
    "params.load_all()\n",
    "print(params)\n",
    "query = Prompt(\"ExampleVectorSearchQuery\", text=\"What's Quinalin's favorite color?\")\n",
    "searcher = VectorSearch(\"ExamplePineconeSearch\", params_name=\"ExamplePineconeParams\", prompt_name=\"ExampleVectorSearchQuery\")\n",
    "searcher.load_all()\n",
    "res = await vectorsearch_query(searcher)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = VectorSearch.objects.get(\"ExamplePineconeSearch\")\n",
    "searcher.load_all()\n",
    "res = await vectorsearch_query(searcher)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plunkylib import *\n",
    "# Testing how nested yaml chat prompts work (i.e. chains)\n",
    "petition_example = Petition.objects.get(\"ExampleChatYmlNestedVSearch\")\n",
    "petition_example.load_all()\n",
    "print(petition_example)\n",
    "example_completion6, adj_prompt_text = await petition_completion2(petition=petition_example, additional={}, content_filter_check=False)\n",
    "print(example_completion6.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa9af9f86d1d854ae0204e4baada49d6c0051b058ab6ea52d61832c609c5af7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
